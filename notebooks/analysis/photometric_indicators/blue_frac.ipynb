{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Physics-related Packages\n",
    "from astropy.cosmology import Planck15 as cosmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparations for read box info from the url\n",
    "import requests\n",
    "import time\n",
    "\n",
    "baseUrl = 'http://www.tng-project.org/api/'\n",
    "\n",
    "def get(path, params=None, max_retries=5, backoff_factor=2):\n",
    "    # make HTTP GET request to path\n",
    "    headers = {\"api-key\":\"API KEY\"}\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            r = requests.get(path, params=params, headers=headers)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            if r.headers['content-type'] == 'application/json':\n",
    "                return r.json()\n",
    "            \n",
    "            if 'content-disposition' in r.headers:\n",
    "                filename = r.headers['content-disposition'].split(\"filename=\")[1]\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return filename\n",
    "            return r  # fallback\n",
    "        \n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            wait = backoff_factor ** attempt\n",
    "            print(f\"[Retry {attempt}/{max_retries}] Request failed: {e}. Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    raise RuntimeError(f\"Failed to GET {path} after {max_retries} retries.\")\n",
    "\n",
    "# Issue a request to the API root\n",
    "r = get(baseUrl)\n",
    "\n",
    "# Print out all the simulation names\n",
    "names = [sim['name'] for sim in r['simulations']]\n",
    "# Get the index of TNG300-1\n",
    "i = names.index('TNG-Cluster')\n",
    "# Get the info of simulation Illustris-3\n",
    "sim = get( r['simulations'][i]['url'] )\n",
    "sim.keys()\n",
    "\n",
    "# get the snaps info this simulation\n",
    "snaps = get(sim['snapshots'])\n",
    "\n",
    "# Sim Box parameters\n",
    "Snap_Index = 99 # the snapshots index in the total 100 snapshots taking at different z\n",
    "BoxSize = sim['boxsize'] # unit: ckpc/h\n",
    "Redshift = snaps[Snap_Index]['redshift'] # current redshift of our current snap\n",
    "\n",
    "# Short description of cosmological parameters Planck2015\n",
    "h_atz = cosmo.H(Redshift).value/100 # unit 100km/[s*Mpc]\n",
    "\n",
    "LowerMass_lim = 10**3 * h_atz # units 10^ 10 ð‘€âŠ™/â„Ž * h\n",
    "HigherMass_lim = 10**8 * h_atz # units 10^ 10 ð‘€âŠ™/â„Ž * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca76b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cat_name = '/users_path/merger_trace/tng_cluster/tng_cluster_targetcat/'\n",
    "\n",
    "mpb_base_path = '/users_path/merger_trace/tng_cluster/tng_cluster_mpbs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6419268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subhalo_maxM(Halo_ID, Sub_GrNr, Sub_Mass):\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds the indices of the top three most massive subhalos for a given halo.\n",
    "\n",
    "    Parameters:\n",
    "    - Halo_ID: Array of halo IDs (single value or array with specific halo ID to match).\n",
    "    - Sub_GrNr: Array indicating the group number (halo) each subhalo belongs to.\n",
    "    - Sub_Mass: Array of subhalo masses.\n",
    "\n",
    "    Returns:\n",
    "    - Numpy array (CenterSub_Index, SecondSub_Index, ThirdSub_Index):\n",
    "      - CenterSub_Index: Index of the most massive subhalo in the halo.\n",
    "      - SecondSub_Index: Index of the second most massive subhalo.\n",
    "      - ThirdSub_Index: Index of the third most massive subhalo.\n",
    "    \"\"\"\n",
    "        \n",
    "    find_Sub = np.where(Sub_GrNr == Halo_ID)[0]\n",
    "    find_Sub_Mass = Sub_Mass[find_Sub]\n",
    "    find_Sub_Mass_Sorted = np.argsort(find_Sub_Mass)\n",
    "\n",
    "    CenterSub_Index = np.where(Sub_GrNr == Halo_ID)[0][find_Sub_Mass_Sorted[-1]]\n",
    "\n",
    "    CenterSub_Index = np.array(CenterSub_Index)\n",
    "    \n",
    "    return CenterSub_Index\n",
    "\n",
    "def Get_AvgSFR(SubhaloGrNr, SubhaloSFR, FOF_Halo_IDs):\n",
    "    AvgSFR = np.zeros(FOF_Halo_IDs.shape)\n",
    "\n",
    "    for i in range(len(FOF_Halo_IDs)):\n",
    "        current_fof_id = FOF_Halo_IDs[i]\n",
    "        sub_in_fof = (SubhaloGrNr == current_fof_id)\n",
    "        subSFR_in_fof = SubhaloSFR[sub_in_fof]\n",
    "        AvgSFR[i] = np.mean(subSFR_in_fof)\n",
    "\n",
    "    return AvgSFR\n",
    "    \n",
    "def Get_HaloIDs(TargetHalo_cat):\n",
    "    \"\"\"\n",
    "    Extracts halo IDs and their properties from the HDF5 catalog.\n",
    "\n",
    "    Parameters:\n",
    "    - TargetHalo_cat: Path to the HDF5 file containing the target halo catalog.\n",
    "\n",
    "    Returns:\n",
    "    - Target_Halo_IDs: List of selected halo IDs.\n",
    "    - Subhalo_MaxMasses: Maximum subhalo masses for each selected halo.\n",
    "    - Target_Halo_Rs_Crit200: Halo critical radius R_Crit200.\n",
    "    - Target_GroupPoses: Positions of the selected halos.\n",
    "    - Galaxy_nums: Number of subhalos per halo.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5py.File(TargetHalo_cat, 'r') as Target_hdf:\n",
    "        # Read FOF Halo Info\n",
    "        FOF_Halo_IDs =  Target_hdf['Group/FOF_Halo_IDs'][:]\n",
    "        GroupFirstSub = Target_hdf['Group/GroupFirstSub'][:]\n",
    "        Group_Nsubs = Target_hdf['Group/GroupNsubs'][:]\n",
    "\n",
    "        # Read Subhalo Info\n",
    "        SubhaloGrNr = Target_hdf['Subhalo/SubhaloGrNr'][:]\n",
    "        SubhaloSFR = Target_hdf['Subhalo/SubhaloSFR'][:]\n",
    "        SubhaloIDs = Target_hdf['Subhalo/Subhalo_IDs'][:]\n",
    "\n",
    "    # Find FOF halos with subhalos and Read Info\n",
    "    Indices_HaloWithSub = np.where( GroupFirstSub != -1)[0]\n",
    "    Target_Halo_IDs = FOF_Halo_IDs[Indices_HaloWithSub]\n",
    "\n",
    "    # Initialize array to get the central subhalo, second massive subhalo, and the third massive subhalo\n",
    "\n",
    "    # Initialize galaxy numbers\n",
    "    Galaxy_nums = Group_Nsubs[np.isin(FOF_Halo_IDs, Target_Halo_IDs)]\n",
    "    AvgSFR = Get_AvgSFR(SubhaloGrNr, SubhaloSFR, FOF_Halo_IDs)\n",
    "\n",
    "    return (Target_Halo_IDs, Galaxy_nums,AvgSFR, SubhaloGrNr, SubhaloIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/users_path/data/tng_cluster/tng_cluster_catalog/TNG-Cluster_Catalog.hdf5', 'r') as f:\n",
    "    origID_parentsim = f['origID'][:]\n",
    "    haloID__TNGCluster = f['haloID'][:]\n",
    "\n",
    "cat_name ='/users_path/data/tng_cluster/tng_cluster_targetcat/targethalo_cat_099/TargetHalo_MergerCat_099.hdf5'\n",
    "\n",
    "results = Get_HaloIDs(cat_name)\n",
    "\n",
    "(Target_Halo_IDs, _,  _, _, _) = results\n",
    "\n",
    "\n",
    "with h5py.File('/users_path/data/tng_cluster/tng_cluster_cluster_mergers/cluster_mergers.hdf5', 'r') as f:\n",
    "    HaloID = f['HaloID'][:]\n",
    "    Snap_coll = f['Snap_coll'][:]\n",
    "    print(f.keys())\n",
    "\n",
    "\n",
    "HaloID_Mergercat = np.array(list(set(HaloID))).astype(int)\n",
    "\n",
    "\n",
    "Merger_Target_Halo_ID = range(len(haloID__TNGCluster))\n",
    "Merger_Target_Halo_ID = np.array(Merger_Target_Halo_ID)\n",
    "\n",
    "Snap_coll_eachhalo = dict()\n",
    "\n",
    "for i in range(len(Merger_Target_Halo_ID)):\n",
    "    current_halo_id = Merger_Target_Halo_ID[i]\n",
    "    rel_merger_snap = Snap_coll[HaloID==current_halo_id]\n",
    "    halo_id_frommergercat2TNGCluster = haloID__TNGCluster[current_halo_id] \n",
    "    Snap_coll_eachhalo[halo_id_frommergercat2TNGCluster] = rel_merger_snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58de439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snaptime(snap):\n",
    "    snap_redshift = snaps[snap]['redshift'] \n",
    "    t_cosmic = cosmo.age(snap_redshift).value  # age of the Universe at that redshift\n",
    "    return t_cosmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45845378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apparent_mag(abs_mag, redshift_snap):\n",
    "    D_L = cosmo.luminosity_distance(redshift_snap).to('pc').value\n",
    "    apparent_mag = abs_mag + 5 * np.log10(D_L / 10)\n",
    "\n",
    "    return apparent_mag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bluefrac(halo_id_snap, cat_name_snap, redshift_snap):\n",
    "    with h5py.File(cat_name_snap, 'r') as f:\n",
    "        FOF_Halo_IDs =  f['Group/FOF_Halo_IDs'][:]\n",
    "        Group_Nsubs =f['Group/GroupNsubs'][:]\n",
    "        SubhaloGrNr = f['Subhalo/SubhaloGrNr'][:]\n",
    "        SubhaloStellarPhotometrics = f['Subhalo/SubhaloStellarPhotometrics'][:] # U, B, V, K, g, r, i, z. \n",
    "        \n",
    "    subhalo_idx = np.where(SubhaloGrNr==halo_id_snap)[0]\n",
    "    assert Group_Nsubs[FOF_Halo_IDs==halo_id_snap] == len(subhalo_idx)\n",
    "    U_band_data = SubhaloStellarPhotometrics[subhalo_idx, 0]\n",
    "    #V_band_data = SubhaloStellarPhotometrics[subhalo_idx, 2]\n",
    "    g_band_data = SubhaloStellarPhotometrics[subhalo_idx, 4]\n",
    "    # make magnitude cut\n",
    "    apparent_U_mag = get_apparent_mag(U_band_data, redshift_snap)\n",
    "    #apparent_V_mag = get_apparent_mag(V_band_data, redshift_snap)\n",
    "    apparent_g_mag = get_apparent_mag(g_band_data, redshift_snap)\n",
    "    #mask = np.where((apparent_U_mag<=22) & (apparent_V_mag<=22))[0]\n",
    "    mask = np.where((apparent_U_mag<=23) & (apparent_g_mag<=25))[0]\n",
    "    visible_number = len(mask)\n",
    "    U_band_masked = U_band_data[mask]\n",
    "    # V_band_masked = V_band_data[mask]\n",
    "    g_band_masked = g_band_data[mask]\n",
    "    blue_galaxy_number_05 = np.sum((U_band_masked - g_band_masked)<=0.5)\n",
    "    blue_galaxy_number_03 = np.sum((U_band_masked - g_band_masked )<=0.3)\n",
    "\n",
    "\n",
    "    blue_frac_05 = blue_galaxy_number_05/visible_number\n",
    "    blue_frac_03 = blue_galaxy_number_03/visible_number\n",
    "\n",
    "    print(visible_number)\n",
    "\n",
    "    return blue_frac_05, blue_frac_03\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7d0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_fromdict(halo_id_99, center_snap, features_dict):\n",
    "\n",
    "    halo_data = features_dict[halo_id_99][center_snap]\n",
    "    if 'label_score_all_tau2.0' not in halo_data  or 'label_score_pre_tau2.0' not in halo_data:\n",
    "        return None, None  \n",
    "\n",
    "    all_merger_score = halo_data['label_score_all_tau2.0']\n",
    "\n",
    "    pre_merger_score = halo_data['label_score_pre_tau2.0']\n",
    "\n",
    "    post_merger_score = all_merger_score-pre_merger_score\n",
    "\n",
    "    return pre_merger_score, post_merger_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/users_path/merger_trace/data/tng_cluster/tng_cluster_products/feats_labels_dict_tngcluster.pkl', 'rb') as f:\n",
    "    features_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a02409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MergerHaloID_TNGCluster = np.array([haloID__TNGCluster[i] for i in HaloID]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_name_99 = base_cat_name + 'targethalo_cat_099/TargetHalo_MergerCat_099.hdf5'\n",
    "\n",
    "Target_Halo_IDs_99, Galaxy_nums_99, AvgSFR_99, SubhaloGrNr_99, SubhaloIDs_99= Get_HaloIDs(cat_name_99)\n",
    "\n",
    "rows = [] \n",
    "for snap in range(72, 100):\n",
    "    redshift_snap = snaps[snap]['redshift']\n",
    "    if redshift_snap<=0.002:\n",
    "        redshift_snap = 0.0025\n",
    "    for fof_halo_id_99 in Snap_coll_eachhalo.keys():\n",
    "        sub1st_at99 = int(SubhaloIDs_99[SubhaloGrNr_99==fof_halo_id_99][0])\n",
    "\n",
    "        with h5py.File(mpb_base_path+f'sublink_mpb_{sub1st_at99}.hdf5', 'r') as mpb_f:\n",
    "            SubfindID = mpb_f['SubfindID'][:]\n",
    "            SnapNum = mpb_f['SnapNum'][:]\n",
    "\n",
    "        cat_name_snap = base_cat_name + f'targethalo_cat_0{snap}/TargetHalo_MergerCat_0{snap}.hdf5'\n",
    "\n",
    "        Target_Halo_IDs_snap, Galaxy_nums_snap, AvgSFR_snap, SubhaloGrNr_snap, SubhaloIDs_snap= Get_HaloIDs(cat_name_snap)\n",
    "        \n",
    "        progenitor_id_snap = SubfindID[SnapNum==snap]\n",
    "\n",
    "        if len(progenitor_id_snap)==0:\n",
    "            continue\n",
    "\n",
    "        halo_id_snap = int(SubhaloGrNr_snap[SubhaloIDs_snap==progenitor_id_snap][0])\n",
    "     \n",
    "        blue_frac_05, blue_frac_03 = get_bluefrac(halo_id_snap, cat_name_snap, redshift_snap)\n",
    "\n",
    "        pre_merger_score, post_merger_score = get_scores_fromdict(fof_halo_id_99, snap, features_dict)\n",
    "        print(pre_merger_score, post_merger_score)\n",
    "        meta_info = (fof_halo_id_99, snap)\n",
    "\n",
    "        rows.append({\n",
    "            \"fof_halo_id_99\": fof_halo_id_99,\n",
    "            \"snap\": snap,\n",
    "            \"halo_id_snap\": halo_id_snap,\n",
    "            \"blue_frac_05\": blue_frac_05,\n",
    "            \"blue_frac_03\": blue_frac_03,\n",
    "            \"merger_score_pre\": pre_merger_score,\n",
    "            \"merger_score_post\": post_merger_score\n",
    "        })\n",
    "\n",
    "        print(f'finish {fof_halo_id_99} at snap {snap} where it was {halo_id_snap} with {blue_frac_03}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6514ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"blue_frac_merger_score_ug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc6344",
   "metadata": {},
   "source": [
    "## pre merger\n",
    "    \n",
    "    \n",
    "plot the correlation between the pre-merger score and blue galaxy fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee85398",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"/home/chuiyang/merger_trace/notebooks/analysis/blue_frac_merger_score_ug.csv\")\n",
    "\n",
    "pre_merger_score_vec = df['merger_score_pre'].values\n",
    "redshift_vec = df['snap'].values\n",
    "blue_frac_vec = df['blue_frac_03'].values\n",
    "post_merger_score_vec = df['merger_score_post'].values\n",
    "\n",
    "mask = (redshift_vec<=99)\n",
    "\n",
    "pre_merger_score_vec = pre_merger_score_vec[mask]\n",
    "redshift_vec = redshift_vec[mask]\n",
    "blue_frac_vec = blue_frac_vec[mask]\n",
    "post_merger_score_vec = post_merger_score_vec[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bins = np.array([72., 77., 82., 87., 93., 100.])# 5 bins for redshift\n",
    "score_bins = np.percentile(pre_merger_score_vec, [0, 20, 40, 60, 80, 100])\n",
    "\n",
    "\n",
    "# created 2D heatmap\n",
    "heatmap = np.full((len(z_bins)-1, len(score_bins)-1), np.nan)\n",
    "# add std matrix\n",
    "std_map = np.full((len(z_bins) - 1, len(score_bins) - 1), np.nan)\n",
    "std_N_map = np.full((len(z_bins) - 1, len(score_bins) - 1), np.nan)\n",
    "sample_count = np.zeros_like(heatmap, dtype=int)\n",
    "\n",
    "for i in range(len(z_bins) - 1):\n",
    "    for j in range(len(score_bins) - 1):\n",
    "        mask = (\n",
    "            (redshift_vec >= z_bins[i]) & (redshift_vec < z_bins[i+1]) &\n",
    "            (pre_merger_score_vec >= score_bins[j]) & (pre_merger_score_vec < score_bins[j+1])\n",
    "        )\n",
    "        values = blue_frac_vec[mask]\n",
    "        sample_count[i, j] = np.sum(mask)\n",
    "        if sample_count[i, j] > 0:\n",
    "            heatmap[i, j] = np.mean(values)\n",
    "            std_map[i, j] = np.std(values)\n",
    "            std_N_map[i, j] = np.std(values)/np.sqrt(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d474e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_centers = [(score_bins[i] + score_bins[i+1]) / 2 for i in range(len(score_bins)-1)]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(z_bins)-1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(z_bins) - 1):\n",
    "    mean = heatmap[i]\n",
    "    std = std_N_map[i]\n",
    "    label = f'{z_bins[i]:.0f}-{z_bins[i+1]:.0f} z'\n",
    "    \n",
    "    plt.errorbar(score_centers, mean, yerr=std, label=label,\n",
    "                 fmt='-o', capsize=4, color=colors[i])\n",
    "\n",
    "plt.xlabel('Pre-merger Score')\n",
    "plt.ylabel('Blue Fraction')\n",
    "plt.title('Blue Fraction vs. Pre-merger Score (with std error bars)')\n",
    "plt.legend(title='Redshift Bins')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ea35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_labels = [f'{z_bins[i]:.0f}-{z_bins[i+1]:.0f} z' for i in range(len(z_bins)-1)]\n",
    "score_labels = list(score_centers)\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in range(len(z_bins)-1):\n",
    "    for j in range(len(score_bins)-1):\n",
    "        data.append({\n",
    "            'z_bin': z_labels[i],\n",
    "            'score_bin': score_labels[j],\n",
    "            'mean': heatmap[i, j],\n",
    "            'std_error': std_N_map[i, j],\n",
    "            'sample_count': sample_count[i, j]\n",
    "        })\n",
    "\n",
    "# save data for plots\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(\"blue_frac_plot_data_pre_ug.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4004f",
   "metadata": {},
   "source": [
    "## post megrer\n",
    "\n",
    "plot the correlation between the post-merger score and blue galaxy fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1517c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/chuiyang/merger_trace/notebooks/analysis/blue_frac_merger_score_ug.csv\")\n",
    "\n",
    "redshift_vec_all = df['snap'].values\n",
    "blue_frac_vec_all = df['blue_frac_03'].values\n",
    "post_merger_score_vec_all = df['merger_score_post'].values\n",
    "\n",
    "# cut snapshots near redshift 0 since they lack future merger information\n",
    "snap_cut = 87\n",
    "mask = redshift_vec_all<=87\n",
    "\n",
    "redshift_vec = redshift_vec_all[mask]\n",
    "blue_frac_vec = blue_frac_vec_all[mask]\n",
    "post_merger_score_vec = post_merger_score_vec_all[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceba030",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_bins = np.linspace(min(redshift_vec), max(redshift_vec), 4)  # 5 bins for redshift\n",
    "score_bins = np.percentile(post_merger_score_vec, [0, 20, 40, 60, 80, 100])\n",
    "\n",
    "heatmap = np.full((len(z_bins)-1, len(score_bins)-1), np.nan)\n",
    "\n",
    "std_map = np.full((len(z_bins) - 1, len(score_bins) - 1), np.nan)\n",
    "std_N_map = np.full((len(z_bins) - 1, len(score_bins) - 1), np.nan)\n",
    "sample_count = np.zeros_like(heatmap, dtype=int)\n",
    "\n",
    "for i in range(len(z_bins) - 1):\n",
    "    for j in range(len(score_bins) - 1):\n",
    "        mask = (\n",
    "            (redshift_vec >= z_bins[i]) & (redshift_vec < z_bins[i+1]) &\n",
    "            (post_merger_score_vec >= score_bins[j]) & (post_merger_score_vec < score_bins[j+1])\n",
    "        )\n",
    "        values = blue_frac_vec[mask]\n",
    "        sample_count[i, j] = np.sum(mask)\n",
    "        if sample_count[i, j] > 0:\n",
    "            heatmap[i, j] = np.mean(values)\n",
    "            std_map[i, j] = np.std(values)\n",
    "            std_N_map[i, j] = np.std(values)/np.sqrt(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8d81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_centers = [(score_bins[i] + score_bins[i+1]) / 2 for i in range(len(score_bins)-1)]\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(z_bins)-1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for i in range(len(z_bins) - 1):\n",
    "    mean = heatmap[i]\n",
    "    std = std_N_map[i]\n",
    "    label = f'{z_bins[i]:.0f}-{z_bins[i+1]:.0f} z'\n",
    "    \n",
    "    plt.errorbar(score_centers, mean, yerr=std, label=label,\n",
    "                 fmt='-o', capsize=4, color=colors[i])\n",
    "\n",
    "plt.xlabel('Pre-merger Score')\n",
    "plt.ylabel('Blue Fraction')\n",
    "plt.title('Blue Fraction vs. Post-merger Score (with std error bars)')\n",
    "plt.legend(title='Redshift Bins')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22f7327",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_labels = [f'{z_bins[i]:.0f}-{z_bins[i+1]:.0f} z' for i in range(len(z_bins)-1)]\n",
    "score_labels = list(score_centers)\n",
    "\n",
    "data = []\n",
    "for i in range(len(z_bins)-1):\n",
    "    for j in range(len(score_bins)-1):\n",
    "        data.append({\n",
    "            'z_bin': z_labels[i],\n",
    "            'score_bin': score_labels[j],\n",
    "            'mean': heatmap[i, j],\n",
    "            'std_error': std_N_map[i, j],\n",
    "            'sample_count': sample_count[i, j]\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(\"blue_frac_plot_data_post_ug.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Illustris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
