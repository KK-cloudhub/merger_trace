{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d56f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress, pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import pickle\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pickle\n",
    "import random\n",
    "# Physics-related Packages\n",
    "from astropy.cosmology import Planck15\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import astropy.units as u\n",
    "from scipy.optimize import root_scalar\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cfbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparations for read box info from the url\n",
    "import requests\n",
    "import time\n",
    "\n",
    "baseUrl = 'http://www.tng-project.org/api/'\n",
    "\n",
    "def get(path, params=None, max_retries=5, backoff_factor=2):\n",
    "    # make HTTP GET request to path\n",
    "    headers = {\"api-key\":\"API KEY\"}\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            r = requests.get(path, params=params, headers=headers)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            if r.headers['content-type'] == 'application/json':\n",
    "                return r.json()\n",
    "            \n",
    "            if 'content-disposition' in r.headers:\n",
    "                filename = r.headers['content-disposition'].split(\"filename=\")[1]\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return filename\n",
    "            return r  # fallback\n",
    "        \n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            wait = backoff_factor ** attempt\n",
    "            print(f\"[Retry {attempt}/{max_retries}] Request failed: {e}. Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    raise RuntimeError(f\"Failed to GET {path} after {max_retries} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue a request to the API root\n",
    "r = get(baseUrl)\n",
    "\n",
    "# Print out all the simulation names\n",
    "names = [sim['name'] for sim in r['simulations']]\n",
    "# Get the index of TNG300-1\n",
    "i = names.index('TNG-Cluster')\n",
    "# Get the info of simulation Illustris-3\n",
    "sim = get( r['simulations'][i]['url'] )\n",
    "sim.keys()\n",
    "\n",
    "# get the snaps info this simulation\n",
    "snaps = get(sim['snapshots'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_redshift_from_lookback_time(tau_gyr):\n",
    "    \"\"\"\n",
    "    Given a look back time, return the redshift\n",
    "    \"\"\"\n",
    "\n",
    "    if tau_gyr >= Planck15.lookback_time(0.01).value:\n",
    "        tau = tau_gyr * u.Gyr\n",
    "\n",
    "\n",
    "        def f(z):\n",
    "            return (Planck15.lookback_time(z) - tau).value  \n",
    "\n",
    "\n",
    "        result = root_scalar(f, bracket=[0.01, 20], method='brentq')\n",
    "        \n",
    "        if result.converged:\n",
    "            return result.root\n",
    "        else:\n",
    "            raise RuntimeError(\"Root finding failed for tau = {:.2f} Gyr\".format(tau_gyr))\n",
    "    \n",
    "    else:\n",
    "        return 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e87794",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_numbers = np.zeros(len(snaps))\n",
    "snap_redshifts = np.zeros(len(snaps))\n",
    "for i, snap in enumerate(snaps):\n",
    "    snap_numbers[i] = snap['number']\n",
    "    snap_redshifts[i] = snap['redshift']\n",
    "\n",
    "def find_nearest_snap(z):\n",
    "    delta_z = np.abs(snap_redshifts - z)\n",
    "    target_snap = snap_numbers[np.argmin(delta_z)]\n",
    "    return int(target_snap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f35e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_feats_labels_to_training_set_snapcut_tv(feats_labels_dict, snap_cut, tau):\n",
    "    feature_vectors = []\n",
    "    label_scores = []\n",
    "    label_scores_pre = []\n",
    "    meta_info = []  \n",
    "\n",
    "    for halo_id in feats_labels_dict:\n",
    "        # print(halo_id)\n",
    "        for snap in feats_labels_dict[halo_id]:\n",
    "            projections = ['xy', 'blank', 'yz', 'blank', 'xz', 'blank']\n",
    "            #projections = ['yz', 'blank', 'xz', 'blank', 'xy', 'blank']\n",
    "            index = np.mod(snap-72,6)\n",
    "            proj = projections[index]\n",
    "            if proj == 'blank':\n",
    "                continue\n",
    "            if snap <= snap_cut:\n",
    "                # print(halo_id,'*',snap)\n",
    "                Redshift = snaps[snap]['redshift'] \n",
    "                h_at0 = Planck15.h # unit 100km/[s*Mpc]\n",
    "                scale = 1 / (h_at0 * (1 + Redshift))\n",
    "                \n",
    "                if f'label_score_all_tau{tau}' not in feats_labels_dict[halo_id][snap]:\n",
    "                    print(f'label_scores_tau{tau} for halo {halo_id} and snap {snap}','vansish')\n",
    "                    continue\n",
    "                label = feats_labels_dict[halo_id][snap][f'label_score_all_tau{tau}']\n",
    "                label_pre = feats_labels_dict[halo_id][snap][f'label_score_pre_tau{tau}']\n",
    "                #print(snap)\n",
    "                projections = feats_labels_dict[halo_id][snap]['features']\n",
    "                \n",
    "\n",
    "                feats = projections[proj]\n",
    "\n",
    "                # Convert dict values to a fixed-order feature vector\n",
    "                vec = [\n",
    "                   # (feats['mean_r_0'] - feats['mean_r_1']) * scale, #0\n",
    "                   # feats['mean_v_0'], #1\n",
    "                   # feats['mean_v_1'], #2\n",
    "                    np.abs(feats['mean_v_0']-feats['mean_v_1']), #3\n",
    "                    feats['std_r_0']* scale, #4\n",
    "                    feats['std_r_1']* scale, #5\n",
    "                    feats['std_v_0'], #6\n",
    "                    feats['std_v_1'], #7\n",
    "                   # min(feats['n0'], feats['n1'])/max(feats['n0'], feats['n1']), #8\n",
    "                   # feats['bic_1'], #9\n",
    "                   # feats['bic_2'], #10\n",
    "                    feats['bic_2'] - feats['bic_1'] - 6*np.log(feats['n0']+ feats['n1']), #11\n",
    "                    feats['elongation_ratio'],  # 12\n",
    "                    feats_labels_dict[halo_id][snap]['redshift'], #13\n",
    "                    feats_labels_dict[halo_id][snap]['mass_ratio'], #14\n",
    "                    feats['std_r'] * scale, #15\n",
    "                    feats['std_v'], #16\n",
    "                ]\n",
    "                #print(feats['n0']+ feats['n1'])\n",
    "                feature_vectors.append(vec)\n",
    "                label_scores.append(label)\n",
    "                label_scores_pre.append(label_pre)\n",
    "                meta_info.append((halo_id, snap, proj))\n",
    "\n",
    "    return np.array(feature_vectors), np.array(label_scores), np.array(label_scores_pre), meta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_label(snapcut, tau):\n",
    "    with open('/user_path/merger_trace/data/tng_cluster/tng_cluster_products/feats_labels_dict_tngcluster.pkl', 'rb') as f:\n",
    "        feats_labels_dict = pickle.load(f)\n",
    "\n",
    "    feature_vectors, label_scores, label_scores_pre, meta_info = convert_feats_labels_to_training_set_snapcut_tv(feats_labels_dict, snapcut, tau)\n",
    "\n",
    "    return feature_vectors, label_scores, label_scores_pre, meta_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c70c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGboost_singletau_premerger_allmerger(tau, state = 'pre_merger', random_state = 42):\n",
    "    if state == 'pre_merger':\n",
    "        snapcut  = 100\n",
    "        feature_vectors, label_scores, label_scores_pre, meta_info = get_feats_label(snapcut, tau)\n",
    "        label_score_usedfor = label_scores_pre\n",
    "\n",
    "    elif state == 'all_merger':\n",
    "        cut_z = find_redshift_from_lookback_time(tau)\n",
    "        snapcut = find_nearest_snap(cut_z)\n",
    "        feature_vectors, label_scores, label_scores_pre, meta_info = get_feats_label(snapcut, tau)\n",
    "        label_score_usedfor = label_scores\n",
    "\n",
    "    \n",
    "    elif state == 'post_merger':\n",
    "        cut_z = find_redshift_from_lookback_time(tau)\n",
    "        snapcut = find_nearest_snap(cut_z)\n",
    "        feature_vectors, label_scores, label_scores_pre, meta_info = get_feats_label(snapcut, tau)\n",
    "        label_score_usedfor = label_scores-label_scores_pre\n",
    "    \n",
    "    else:\n",
    "        print('Please choose the right version of label scores')\n",
    "    \n",
    "    mask = np.where((label_score_usedfor >= 0 )& (label_score_usedfor <=20))[0].astype(int)\n",
    "\n",
    "    # grouping data\n",
    "    groups = [f\"{hid}_{snap}\" for (hid, snap, proj) in np.array(meta_info)[mask]]\n",
    "    n_samples = len(feature_vectors[mask])\n",
    "    assert len(label_score_usedfor[mask]) == n_samples == len(groups)\n",
    "\n",
    "    # split training data and test data\n",
    "    X = np.arange(len(groups))\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "    train_idx, test_idx = next(gss.split(X, label_score_usedfor[mask], groups=groups))\n",
    "\n",
    "    X_train, X_test = feature_vectors[mask][train_idx], feature_vectors[mask][test_idx]\n",
    "    y_train, y_test = label_score_usedfor[mask][train_idx], label_score_usedfor[mask][test_idx]\n",
    "    group_train = np.array(groups)[train_idx]  # for GridSearchCV\n",
    "\n",
    "    # set search grid for hyper parameters\n",
    "    param_grid = {\n",
    "    'learning_rate': [0.005,0.01,0.05, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 4, 6, 8, 10],\n",
    "    'n_estimators': [100, 200, 300, 500, 1000],\n",
    "    }\n",
    "\n",
    "    #initialize model and grid search\n",
    "    model = XGBRegressor(random_state=random_state)\n",
    "    cv_splitter = GroupShuffleSplit(n_splits=3, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='r2',\n",
    "    cv=cv_splitter,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train, groups=group_train)\n",
    "\n",
    "    # predict using best model \n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Best Params: {grid.best_params_}\")\n",
    "    print(f\"XGBoost MSE: {mse:.4f}\")\n",
    "    print(f\"XGBoost R²: {r2:.4f}\")\n",
    "\n",
    "    # visualize data\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, edgecolors='k', linewidths=0.3)\n",
    "    plt.plot([0, 5], [0, 5], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('True Merger Score')\n",
    "    plt.ylabel('Predicted Merger Score')\n",
    "    plt.title('XGBoost: True vs Predicted')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    print(f\"Training R²: {r2_train:.4f}\")\n",
    "    print(f\"Training MSE: {mse_train:.4f}\")\n",
    "\n",
    "    feature_names = [\n",
    "    #'delta_r_scaled', 'mean_v_0', 'mean_v_1', \n",
    "    'delta_v_abs',\n",
    "    'std_r_0', 'std_r_1', 'std_v_0', 'std_v_1',\n",
    "    #'number_ratio', 'bic_1', 'bic_2', \n",
    "    'bic_penalized_diff',\n",
    "    'elongation_ratio', 'redshift', 'mass_ratio', 'std_r', 'std_v'\n",
    "    ]\n",
    "\n",
    "\n",
    "    # ==== Permutation importance ====\n",
    "    from sklearn.inspection import permutation_importance\n",
    "\n",
    "    print(\"\\n--- Computing Permutation Importance ---\")\n",
    "    perm_result = permutation_importance(\n",
    "        best_model, X_test, y_test, n_repeats=10, random_state=random_state, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    perm_sorted_idx = np.argsort(perm_result.importances_mean)[::-1]\n",
    "    plt.figure(figsize=(6.5, 4.5))\n",
    "    plt.bar(range(len(perm_sorted_idx)), perm_result.importances_mean[perm_sorted_idx], color=\"#d95f02\")\n",
    "    plt.xticks(range(len(perm_sorted_idx)), np.array(feature_names)[perm_sorted_idx], rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.ylabel(\"Permutation Importance\", fontsize=12)\n",
    "    plt.title(\"XGBoost Permutation Importance\", fontsize=13)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return (mse, r2, mse_train, r2_train, n_samples, y_test, y_pred, label_score_usedfor[mask],\n",
    "            perm_result.importances_mean, perm_result.importances_std, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3599fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = round(2.0, 1)\n",
    "random_state = 2025\n",
    "mse, r2, mse_train, r2_train, n_samples, y_test, y_pred, score_distribution, importances_mean, importances_std, feature_names = XGboost_singletau_premerger_allmerger(tau, state = 'post_merger', random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d2020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "np.savez('score_distribution_rotate_discussion_tau2.0_premerger.npz',\n",
    "        scores=score_distribution)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"scatter_rotate_discussion_tau2.0_postmerger.npz\", y_test=y_test, y_pred=y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18440e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with feature names, importance means, and std deviations\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance Mean': importances_mean,\n",
    "    'Importance Std': importances_std\n",
    "})\n",
    "\n",
    "# Save the results to a CSV file\n",
    "importance_df.to_csv('permutation_importance_rotate_discussion_tau2.0_postmerger.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4.5))\n",
    "plt.hist(score_distribution, bins=30, color='gray', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Merger Score', fontsize=12)\n",
    "plt.ylabel('Halo Count', fontsize=12)\n",
    "plt.title(r'Score Distribution at $\\tau = {}$ Gyr'.format(tau), fontsize=13)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "label_variance = np.var(y_test)\n",
    "print(f\"Var(y_test) = {label_variance:.4f}\")\n",
    "print(f\"MSE / Var = {mse / label_variance:.3f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Illustris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
