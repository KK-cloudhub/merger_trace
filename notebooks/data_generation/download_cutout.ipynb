{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fbd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress, pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "# Physics-related Packages\n",
    "from astropy.cosmology import Planck15\n",
    "from astropy.cosmology import Planck15 as cosmo\n",
    "from scipy.optimize import brentq\n",
    "import astropy.units as u\n",
    "from numpy.polynomial.polynomial import polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd360f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "baseUrl = 'http://www.tng-project.org/api/'\n",
    "headers = {\"api-key\": \"API KEY\"}\n",
    "\n",
    "def get(path, params=None, max_retries=5, retry_delay=2):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.get(path, params=params, headers=headers, timeout=30)\n",
    "            r.raise_for_status()  \n",
    "            if r.headers.get('content-type') == 'application/json':\n",
    "                return r.json()\n",
    "            if 'content-disposition' in r.headers:\n",
    "                filename = r.headers['content-disposition'].split(\"filename=\")[1]\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return filename\n",
    "            return r  # fallback\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ConnectionError) as e:\n",
    "            print(f\"Attempt {attempt+1}/{max_retries} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                raise  \n",
    "\n",
    "# Issue a request to the API root\n",
    "r = get(baseUrl)\n",
    "\n",
    "# Print out all the simulation names\n",
    "names = [sim['name'] for sim in r['simulations']]\n",
    "# Get the index of TNG300-1\n",
    "i = names.index('TNG-Cluster')\n",
    "# Get the info of simulation Illustris-3\n",
    "sim = get( r['simulations'][i]['url'] )\n",
    "sim.keys()\n",
    "\n",
    "# get the snaps info this simulation\n",
    "snaps = get(sim['snapshots'])\n",
    "\n",
    "# Sim Box parameters\n",
    "Snap_Index = 99 # the snapshots index in the total 100 snapshots taking at different z\n",
    "BoxSize = sim['boxsize'] # unit: ckpc/h\n",
    "Redshift = snaps[Snap_Index]['redshift'] # current redshift of our current snap\n",
    "\n",
    "# Short description of cosmological parameters Planck2015\n",
    "h_atz = Planck15.H(Redshift).value/100 # unit 100km/[s*Mpc]\n",
    "\n",
    "LowerMass_lim = 10**3 * h_atz # units 10^ 10 ð‘€âŠ™/â„Ž * h\n",
    "HigherMass_lim = 10**10 * h_atz # units 10^ 10 ð‘€âŠ™/â„Ž * h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02939a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_1 = ['Coordinates', 'Density', 'ElectronAbundance', 'EnergyDissipation', 'GFM_AGNRadiation']\n",
    "fields_2 = ['GFM_CoolingRate', 'GFM_Metallicity', 'GFM_Metals', 'InternalEnergy']\n",
    "fields_3 = ['Masses','StarFormationRate','Velocities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d3e3e-3be4-4ffd-bb51-010bd18f5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = pd.read_csv('/users/ckong13/data/Chuiyang/TNGCluster_Cutout/fof_halo_to_sub84.csv')\n",
    "Subhalo_IDs_atTargetSnap = cat_df['Subhalo_ID_At84']\n",
    "FOF_IDs_PRESORTED = cat_df['FOF_Halo_ID_At84']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22efca-50c3-46df-8f87-120a91e2f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_halos = np.array([11314416])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "start_all = time.time() \n",
    "snap = 84\n",
    "field_groups = [fields_1, fields_2, fields_3]\n",
    "\n",
    "for current_subhalo_id in missing_halos:\n",
    "# for i,current_subhalo_id in enumerate(Subhalo_IDs_atTargetSnap):\n",
    "    print(f\"ðŸŸ¡ Start downloading {current_subhalo_id}...\")\n",
    "\n",
    "    try:\n",
    "        #fof_id_presorted = FOF_IDs_PRESORTED[i]\n",
    "        sub_url_base = f\"https://www.illustris-project.org/api/TNG-Cluster/snapshots/{snap}/subhalos/{current_subhalo_id}/\"\n",
    "        sub_info = get(sub_url_base)\n",
    "        FOF_cutout_url = sub_info['cutouts']['parent_halo']\n",
    "        match = re.search(r'/halos/(\\d+)/', FOF_cutout_url)\n",
    "        fof_id = int(match.group(1))\n",
    "        #if fof_id != fof_id_presorted:\n",
    "        #    print(f'halo {current_subhalo_id} invalid')\n",
    "        filenames = []\n",
    "\n",
    "        # dowload data in chunks and then merge data\n",
    "        for idx, fields in enumerate(field_groups):\n",
    "            url = FOF_cutout_url + \"?gas=\" + \",\".join(fields)\n",
    "            fname = f\"cutout_{current_subhalo_id}_part{idx+1}.hdf5\"\n",
    "            r = requests.get(url, headers=headers, stream=True)\n",
    "            r.raise_for_status()\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            filenames.append(fname)\n",
    "\n",
    "        # combine different files\n",
    "        final_fname = f\"cutout_sub{current_subhalo_id}_FOF{fof_id}.hdf5\"\n",
    "        with h5py.File(final_fname, 'w') as fout:\n",
    "            for fname in filenames:\n",
    "                with h5py.File(fname, 'r') as fin:\n",
    "                    for key in fin['PartType0'].keys():\n",
    "                        fout.create_dataset(f'PartType0/{key}', data=fin['PartType0'][key][:])\n",
    "\n",
    "        # remover intermediate files\n",
    "        for fname in filenames:\n",
    "            os.remove(fname)\n",
    "\n",
    "        print(f\"Finished downloading and merging for {current_subhalo_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {current_subhalo_id} due to {e}\")\n",
    "\n",
    "# running time\n",
    "elapsed_all = time.time() - start_all\n",
    "print(f\"\\n All downloads finished in {elapsed_all:.2f} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
