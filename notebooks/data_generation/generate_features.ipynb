{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74578d3a",
   "metadata": {},
   "source": [
    "# generate_features.ipynb\n",
    "\n",
    "This notebook is designed to generate features used for machine learning models that characterize structural properties of galaxy clusters in the TNG-Cluster simulation.  \n",
    "It processes group catalogs and merger trees to produce inputs for machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## Main Functions\n",
    "\n",
    "- Load group catalogs and cluster merger event files from TNG-Cluster.\n",
    "- Track primary halos and extract main progenitor information using merger trees.\n",
    "- Fit galaxy distributions in phase space using Gaussian Mixture Models (GMM).\n",
    "- Adopt 2-component GMM to model major/minor substructure; features are extracted to quantify asymmetry, kinematics, and morphology.\n",
    "- Save feature and label dictionaries for ML training and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Input Requirements\n",
    "\n",
    "- **Local access to**:\n",
    "  - TNG-Cluster merger event file (`cluster_mergers.hdf5`)\n",
    "  - Sorted group catalog (`TargetHalo_MergerCat_099.hdf5`)\n",
    "  - TNG-Cluster catalog (`TNG-Cluster_Catalog.hdf5`)\n",
    "  - Sublink merger trees (`tng_cluster_mpbs/*.hdf5`)\n",
    "\n",
    "- Required parameters such as snapshot number (`snapnum=99`) should match the filenames.\n",
    "\n",
    "- All file paths must be modified from placeholder (`users_path/...`) to your actual local structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "- A pickled `.pkl` file containing two dictionaries:\n",
    "  - `feat_dict`: dictionary of feature vectors indexed by halo ID.\n",
    "  - `label_dict`: corresponding classification labels for each halo (e.g., merger/non-merger).\n",
    "\n",
    "- Output is saved to:  \n",
    "  `feats_labels_TNGCluster_fullsample_final.pkl`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb6efb",
   "metadata": {},
   "source": [
    "## Feature Vector Design \n",
    "\n",
    "The structure includes both raw GMM outputs and derived quantities.  \n",
    "Derived features are placed **at the end** to allow easy removal for nonlinear models.\n",
    "The detail of the features would be described in our paper.\n",
    "\n",
    "\n",
    "### Base GMM Output Features (Raw Features)\n",
    "\n",
    "| Feature Name      | Description                                      |\n",
    "|-------------------|--------------------------------------------------|\n",
    "| `n0`, `n1`        | Number of galaxies/subhalos in each component    |\n",
    "| `mean_r_0`, `mean_r_1` | Mean position (projected) in each component  |\n",
    "| `mean_v_0`, `mean_v_1` | Mean velocity (LOS) in each component        |\n",
    "| `std_r_0`, `std_r_1`   | Std. dev. of projected positions              |\n",
    "| `std_v_0`, `std_v_1`   | Std. dev. of line-of-sight velocities         |\n",
    "| `bic_1`, `bic_2`       | Bayesian Information Criterion (BIC) values   |\n",
    "| `elongation_ratio_xy` | $\\lambda_2 / \\lambda_1$ — Flattening ratio in XY-plane from PCA |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac48a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress, pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "# Physics-related Packages\n",
    "from astropy.cosmology import Planck15\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from astropy.cosmology import Planck15 as cosmo\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f350ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subhalo_maxM(Halo_ID, Sub_GrNr, Sub_Mass):\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds the indices of the top three most massive subhalos for a given halo.\n",
    "\n",
    "    Parameters:\n",
    "    - Halo_ID: Array of halo IDs (single value or array with specific halo ID to match).\n",
    "    - Sub_GrNr: Array indicating the group number (halo) each subhalo belongs to.\n",
    "    - Sub_Mass: Array of subhalo masses.\n",
    "\n",
    "    Returns:\n",
    "    - Numpy array (CenterSub_Index, SecondSub_Index, ThirdSub_Index):\n",
    "      - CenterSub_Index: Index of the most massive subhalo in the halo.\n",
    "      - SecondSub_Index: Index of the second most massive subhalo.\n",
    "      - ThirdSub_Index: Index of the third most massive subhalo.\n",
    "    \"\"\"\n",
    "        \n",
    "    find_Sub = np.where(Sub_GrNr == Halo_ID)[0]\n",
    "    find_Sub_Mass = Sub_Mass[find_Sub]\n",
    "    find_Sub_Mass_Sorted = np.argsort(find_Sub_Mass)\n",
    "\n",
    "    CenterSub_Index = np.where(Sub_GrNr == Halo_ID)[0][find_Sub_Mass_Sorted[-1]]\n",
    "\n",
    "    CenterSub_Index = np.array(CenterSub_Index)\n",
    "    \n",
    "    return CenterSub_Index\n",
    "\n",
    "def Get_AvgSFR(SubhaloGrNr, SubhaloSFR, FOF_Halo_IDs):\n",
    "    AvgSFR = np.zeros(FOF_Halo_IDs.shape)\n",
    "\n",
    "    for i in range(len(FOF_Halo_IDs)):\n",
    "        current_fof_id = FOF_Halo_IDs[i]\n",
    "        sub_in_fof = (SubhaloGrNr == current_fof_id)\n",
    "        subSFR_in_fof = SubhaloSFR[sub_in_fof]\n",
    "        AvgSFR[i] = np.mean(subSFR_in_fof)\n",
    "\n",
    "    return AvgSFR\n",
    "    \n",
    "def Get_HaloIDs(TargetHalo_cat, SubhaloMassDef):\n",
    "    \"\"\"\n",
    "    Extracts halo IDs and their properties from the HDF5 catalog.\n",
    "\n",
    "    Parameters:\n",
    "    - TargetHalo_cat: Path to the HDF5 file containing the target halo catalog.\n",
    "\n",
    "    Returns:\n",
    "    - Target_Halo_IDs: List of selected halo IDs.\n",
    "    - Subhalo_MaxMasses: Maximum subhalo masses for each selected halo.\n",
    "    - Target_Halo_Rs_Crit200: Halo critical radius R_Crit200.\n",
    "    - Target_GroupPoses: Positions of the selected halos.\n",
    "    - Galaxy_nums: Number of subhalos per halo.\n",
    "    \"\"\"\n",
    "\n",
    "    with h5py.File(TargetHalo_cat, 'r') as Target_hdf:\n",
    "        # Read FOF Halo Info\n",
    "        FOF_Halo_IDs =  Target_hdf['Group/FOF_Halo_IDs'][:]\n",
    "        GroupFirstSub = Target_hdf['Group/GroupFirstSub'][:]\n",
    "        GroupPos = Target_hdf['Group/GroupPos'][:]\n",
    "        Group_R_Crit200 = Target_hdf['Group/Group_R_Crit200'][:]\n",
    "        Group_Nsubs = Target_hdf['Group/GroupNsubs'][:]\n",
    "\n",
    "        # Read Subhalo Info\n",
    "        SubhaloGrNr = Target_hdf['Subhalo/SubhaloGrNr'][:]\n",
    "        SubhaloMass =  Target_hdf[f'Subhalo/{SubhaloMassDef}'][:]\n",
    "        SubhaloSpin = Target_hdf['Subhalo/SubhaloSpin'][:]\n",
    "        SubhaloVelDisp = Target_hdf['Subhalo/SubhaloVelDisp'][:]\n",
    "        SubhaloSFR = Target_hdf['Subhalo/SubhaloSFR'][:]\n",
    "        Subhalo_IDs = Target_hdf['Subhalo/Subhalo_IDs'][:]\n",
    "        Subhalo_MassRad = 2*Target_hdf['Subhalo/SubhaloHalfmassRad'][:]\n",
    "\n",
    "    # Find FOF halos with subhalos and Read Info\n",
    "    Indices_HaloWithSub = np.where( GroupFirstSub != -1)[0]\n",
    "    Target_Halo_IDs = FOF_Halo_IDs[Indices_HaloWithSub]\n",
    "    Target_Halo_Rs_Crit200 = Group_R_Crit200[Indices_HaloWithSub]\n",
    "    Target_GroupPoses = GroupPos[Indices_HaloWithSub]\n",
    "\n",
    "    # Initialize array to get the central subhalo, second massive subhalo, and the third massive subhalo\n",
    "\n",
    "    # Initialize galaxy numbers\n",
    "    Galaxy_nums = Group_Nsubs[np.isin(FOF_Halo_IDs, Target_Halo_IDs)]\n",
    "    AvgSFR = Get_AvgSFR(SubhaloGrNr, SubhaloSFR, FOF_Halo_IDs)\n",
    "\n",
    "    # Initialize the Center subhalo info of each fof halo (consider it as the FOF halo itself using SubFind algorithms)\n",
    "    Center_SubhaloIDs = np.zeros(Target_Halo_IDs.shape)\n",
    "    Center_SubhaloMasses = np.zeros(Target_Halo_IDs.shape)\n",
    "    Center_SubhaloVelDisp = np.zeros(Target_Halo_IDs.shape)\n",
    "    Center_SubhaloSFR = np.zeros(Target_Halo_IDs.shape)\n",
    "    Center_SubhaloSpin = np.zeros((Target_Halo_IDs.shape[0],3))\n",
    "    Center_SubhaloMassRad = np.zeros(Target_Halo_IDs.shape)\n",
    "      \n",
    "    for i in range(len(Target_Halo_IDs)):\n",
    "        # locate the current FOF Halo ID\n",
    "        Halo_ID = Target_Halo_IDs[i]\n",
    "\n",
    "        # Find the indices of center, second, third subhalos\n",
    "        CenterSub_Index = get_subhalo_maxM(Halo_ID, SubhaloGrNr, SubhaloMass)\n",
    "\n",
    "        Center_SubhaloIDs[i] = Subhalo_IDs[CenterSub_Index]\n",
    "        Center_SubhaloMasses[i] = SubhaloMass[CenterSub_Index]\n",
    "        Center_SubhaloVelDisp[i] = SubhaloVelDisp[CenterSub_Index]\n",
    "        Center_SubhaloSFR[i] = SubhaloSFR[CenterSub_Index]\n",
    "        Center_SubhaloSpin[i] = SubhaloSpin[CenterSub_Index]\n",
    "        Center_SubhaloMassRad[i] = Subhalo_MassRad[CenterSub_Index]\n",
    "\n",
    "    return (Target_Halo_IDs, Galaxy_nums, Target_Halo_Rs_Crit200, Target_GroupPoses, AvgSFR,\n",
    "            Center_SubhaloIDs, Center_SubhaloMasses, Center_SubhaloVelDisp, Center_SubhaloSFR, Center_SubhaloSpin, Center_SubhaloMassRad\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae24b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparations for read box info from the url\n",
    "import requests\n",
    "import time\n",
    "\n",
    "baseUrl = 'http://www.tng-project.org/api/'\n",
    "\n",
    "def get(path, params=None, max_retries=5, backoff_factor=2):\n",
    "    # make HTTP GET request to path\n",
    "    headers = {\"api-key\":\"API KEY\"}\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            r = requests.get(path, params=params, headers=headers)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            if r.headers['content-type'] == 'application/json':\n",
    "                return r.json()\n",
    "            \n",
    "            if 'content-disposition' in r.headers:\n",
    "                filename = r.headers['content-disposition'].split(\"filename=\")[1]\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return filename\n",
    "            return r  # fallback\n",
    "        \n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            wait = backoff_factor ** attempt\n",
    "            print(f\"[Retry {attempt}/{max_retries}] Request failed: {e}. Retrying in {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "    raise RuntimeError(f\"Failed to GET {path} after {max_retries} retries.\")\n",
    "\n",
    "# Issue a request to the API root\n",
    "r = get(baseUrl)\n",
    "\n",
    "# Print out all the simulation names\n",
    "names = [sim['name'] for sim in r['simulations']]\n",
    "# Get the index of TNG300-1\n",
    "i = names.index('TNG-Cluster')\n",
    "# Get the info of simulation Illustris-3\n",
    "sim = get( r['simulations'][i]['url'] )\n",
    "sim.keys()\n",
    "\n",
    "# get the snaps info this simulation\n",
    "snaps = get(sim['snapshots'])\n",
    "\n",
    "# Sim Box parameters\n",
    "Snap_Index = 99 # the snapshots index in the total 100 snapshots taking at different z\n",
    "BoxSize = sim['boxsize'] # unit: ckpc/h\n",
    "Redshift = snaps[Snap_Index]['redshift'] # current redshift of our current snap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e2ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enlongation_projection(relative_Pos, relative_Vel, plane='xy'):\n",
    "    \"\"\"\n",
    "    Project subhalo positions onto elongation axis in given 2D plane.\n",
    "\n",
    "    Parameters:\n",
    "    - relative_Pos: (N, 3)\n",
    "    - relative_Vel: (N, 3)\n",
    "    - plane: 'xy', 'yz', or 'xz'\n",
    "\n",
    "    Returns:\n",
    "    - r_proj: projected position along elongation direction\n",
    "    - v_los: velocity along remaining axis\n",
    "    \"\"\"\n",
    "    if plane == 'xy':\n",
    "        proj = relative_Pos[:, :2]\n",
    "        v_los = relative_Vel[:, 2]\n",
    "    elif plane == 'yz':\n",
    "        proj = relative_Pos[:, 1:]\n",
    "        v_los = relative_Vel[:, 0]\n",
    "    elif plane == 'xz':\n",
    "        proj = relative_Pos[:, [0, 2]]\n",
    "        v_los = relative_Vel[:, 1]\n",
    "    else:\n",
    "        raise ValueError(\"Plane must be one of 'xy', 'yz', or 'xz'.\")\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    axis = pca.fit(proj).components_[0]\n",
    "    r_proj = proj @ axis\n",
    "    return r_proj, v_los\n",
    "\n",
    "\n",
    "def compute_flattening_ratio(relative_Pos, plane='xy'):\n",
    "    \"\"\"\n",
    "    Compute the flattening ratio (λ2 / λ1) from 2D PCA in the specified projection plane.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    relative_Pos : ndarray of shape (N, 3)\n",
    "        Relative positions of subhalos.\n",
    "    plane : str\n",
    "        Projection plane: 'xy', 'yz', or 'xz'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    flattening_ratio : float\n",
    "        Ratio of the second to the first PCA eigenvalue (λ2 / λ1), indicating shape elongation.\n",
    "    \"\"\"\n",
    "\n",
    "    if plane == 'xy':\n",
    "        proj = relative_Pos[:, [0, 1]]\n",
    "    elif plane == 'yz':\n",
    "        proj = relative_Pos[:, [1, 2]]\n",
    "    elif plane == 'xz':\n",
    "        proj = relative_Pos[:, [0, 2]]\n",
    "    else:\n",
    "        raise ValueError(\"Plane must be one of 'xy', 'yz', or 'xz'.\")\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(proj)\n",
    "    λ1, λ2 = pca.explained_variance_  # Already sorted: λ1 ≥ λ2\n",
    "    flattening_ratio = λ2 / (λ1 + 1e-8)  # epsilon to avoid divide-by-zero\n",
    "\n",
    "    return flattening_ratio\n",
    "\n",
    "def get_relative_pos_vel(cat_name, fofhalo_id):\n",
    "    with h5py.File(cat_name, 'r') as f:\n",
    "        SubhaloVel = f['Subhalo/SubhaloVel'][:]\n",
    "        SubhaloGrNr = f['Subhalo/SubhaloGrNr'][:]\n",
    "        SubhaloPos = f['Subhalo/SubhaloPos'][:]\n",
    "        SubhaloMass = f['Subhalo/SubhaloMass'][:]\n",
    "\n",
    "    if np.sum(SubhaloGrNr == fofhalo_id) == 0:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    center_index = np.argmax(SubhaloMass[SubhaloGrNr == fofhalo_id])\n",
    "    center_sub_pos = SubhaloPos[SubhaloGrNr == fofhalo_id][center_index]\n",
    "    center_sub_vel = SubhaloVel[SubhaloGrNr == fofhalo_id][center_index]\n",
    "\n",
    "    relative_Pos = SubhaloPos[SubhaloGrNr == fofhalo_id] - center_sub_pos\n",
    "    relative_Vel = SubhaloVel[SubhaloGrNr == fofhalo_id] - center_sub_vel\n",
    "\n",
    "    return relative_Pos, relative_Vel\n",
    "\n",
    "\n",
    "def plot_phase_space_projections(relative_Pos, relative_Vel, ax=None, if_plot=True):\n",
    "    \"\"\"\n",
    "    Plot 3 standard projections and 1 elongation-based projection of phase space.\n",
    "\n",
    "    Parameters:\n",
    "    - cat_name: path to catalog file\n",
    "    - fofhalo_id: FOF halo ID\n",
    "    - snap_num: snapshot number\n",
    "    - ax: optional matplotlib axes (4 subplots)\n",
    "    - if_plot: whether to plot\n",
    "    - bins: unused (placeholder for velocity histogram)\n",
    "    \"\"\"\n",
    "    # Elongation axis projection\n",
    "    # Elongation axis projection in 3 planes\n",
    "    r_enlong_xy, v_los_xy = get_enlongation_projection(relative_Pos, relative_Vel, plane='xy')\n",
    "    r_enlong_yz, v_los_yz = get_enlongation_projection(relative_Pos, relative_Vel, plane='yz')\n",
    "    r_enlong_xz, v_los_xz = get_enlongation_projection(relative_Pos, relative_Vel, plane='xz')\n",
    "\n",
    "    if ax is None and if_plot:\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(18, 10))  # 6 plots: 3 basic + 3 elongation\n",
    "    \n",
    "    ax = ax.flatten()\n",
    "\n",
    "    if if_plot:\n",
    "        # Elongation-based projections\n",
    "        ax[0].scatter(r_enlong_xy, v_los_xy, s=8, alpha=0.6)\n",
    "        ax[0].set_title(\"Elongation XY\")\n",
    "        ax[0].set_xlabel(\"Elongated R (xy)\")\n",
    "        ax[0].set_ylabel(\"Vz\")\n",
    "\n",
    "        ax[1].scatter(r_enlong_yz, v_los_yz, s=8, alpha=0.6)\n",
    "        ax[1].set_title(\"Elongation YZ\")\n",
    "        ax[1].set_xlabel(\"Elongated R (yz)\")\n",
    "        ax[1].set_ylabel(\"Vx\")\n",
    "\n",
    "        ax[2].scatter(r_enlong_xz, v_los_xz, s=8, alpha=0.6)\n",
    "        ax[2].set_title(\"Elongation XZ\")\n",
    "        ax[2].set_xlabel(\"Elongated R (xz)\")\n",
    "        ax[2].set_ylabel(\"Vy\")\n",
    "\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f30d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gmm_features(R, V, random_state=42):\n",
    "    \"\"\"\n",
    "    Fit GMM to (R, V) phase space and extract component features.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    R, V : array-like\n",
    "        Projected position and LOS velocity.\n",
    "    bic_thresh : float\n",
    "        Threshold for BIC improvement to accept 2-component model.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    features : dict\n",
    "        Dictionary of extracted features from best model.\n",
    "        Returns NaNs if 2-component fit is not valid.\n",
    "    \"\"\"\n",
    "    X = np.vstack([R, V]).T\n",
    "    X = (X - np.mean(X, axis=0)) / (np.std(X, axis=0) + 1e-8)  # z-score normalization\n",
    "    \n",
    "    if len(R) <150:\n",
    "        max_iterations = 150\n",
    "    \n",
    "    else:\n",
    "        max_iterations = 300\n",
    "\n",
    "    gmm1 = GaussianMixture(n_components=1, covariance_type='full',\n",
    "                       max_iter=max_iterations, n_init=5, random_state=random_state).fit(X)\n",
    "    \n",
    "    if not gmm1.converged_:\n",
    "        print(\"GMM1 did not converge, retrying with n_init=5...\")\n",
    "        gmm1 = GaussianMixture(n_components=1, covariance_type='full',\n",
    "                            max_iter=1.5*max_iterations, n_init=10, random_state=random_state).fit(X)\n",
    "\n",
    "    gmm2 = GaussianMixture(n_components=2, covariance_type='full',\n",
    "                       max_iter=max_iterations, n_init=5, random_state=random_state).fit(X)\n",
    "    \n",
    "    if not gmm2.converged_:\n",
    "        print(\"GMM2 did not converge, retrying with n_init=5...\")\n",
    "        gmm2 = GaussianMixture(n_components=2, covariance_type='full',\n",
    "                           max_iter=1.5*max_iterations, n_init=10, random_state=random_state).fit(X)\n",
    "\n",
    "\n",
    "    bic1 = gmm1.bic(X)\n",
    "    bic2 = gmm2.bic(X)\n",
    "\n",
    "    # Valid GMM\n",
    "    labels = gmm2.predict(X)\n",
    "    group0 = (labels == 0)\n",
    "    group1 = (labels == 1)\n",
    "\n",
    "    R0, V0 = R[group0], V[group0]\n",
    "    R1, V1 = R[group1], V[group1]\n",
    "\n",
    "\n",
    "    return {\n",
    "        'bic_1': bic1, 'bic_2': bic2,\n",
    "        'mean_r_0': np.mean(R0), 'mean_r_1': np.mean(R1),\n",
    "        'std_r': np.std(R),\n",
    "        'mean_v_0': np.mean(V0), 'mean_v_1': np.mean(V1),\n",
    "        'std_r_0': np.std(R0),   'std_r_1': np.std(R1),\n",
    "        'std_v_0': np.std(V0),   'std_v_1': np.std(V1),\n",
    "        'std_v': np.std(V),\n",
    "        'n0': len(R0), 'n1': len(R1),\n",
    "\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_features_for_snapshot(cat_name, fofhalo_id, random_state = 42):\n",
    "\n",
    "    relative_Pos, relative_Vel = get_relative_pos_vel(cat_name, fofhalo_id)\n",
    "\n",
    "    if isinstance(relative_Pos, float) and np.isnan(relative_Pos):\n",
    "        return {}\n",
    "\n",
    "    \n",
    "    all_features = {}\n",
    "\n",
    "    for plane in ['xy', 'yz', 'xz']:\n",
    "        # PCA projection onto elongation axis\n",
    "        R_proj, V_los = get_enlongation_projection(relative_Pos, relative_Vel, plane=plane)\n",
    "\n",
    "        flat_ratio = compute_flattening_ratio(relative_Pos, plane=plane)\n",
    "\n",
    "        # GMM feature extraction\n",
    "        gmm_feats = extract_gmm_features(R_proj, V_los, random_state=random_state)\n",
    "\n",
    "        # Append to results\n",
    "        gmm_feats['elongation_ratio'] = flat_ratio\n",
    "        \n",
    "        all_features[plane] = gmm_feats\n",
    "\n",
    "    return all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff16112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snaptime(snap):\n",
    "    snap_redshift = snaps[snap]['redshift'] \n",
    "    t_cosmic = cosmo.age(snap_redshift).value  # age of the Universe at that redshift\n",
    "    return t_cosmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf75807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merger_score_all(\n",
    "    target_halo: int,\n",
    "    snapshot: int,\n",
    "    HaloID: np.ndarray,\n",
    "    Snap_coll: np.ndarray,\n",
    "    N_window: int = 100,\n",
    "    tau: float = 5.0,\n",
    ") -> float:\n",
    "    \n",
    "    merger_snaps = Snap_coll[HaloID == target_halo]\n",
    "    score = 0.0\n",
    "    t_cosmic_current_snap = get_snaptime(snapshot)\n",
    "    if len(merger_snaps)>=1:\n",
    "        for merger_snap in merger_snaps:\n",
    "            if merger_snap in range(snapshot - N_window, snapshot + N_window + 1):\n",
    "                t_cosmic_merger = get_snaptime(merger_snap)\n",
    "                delta_t = np.abs(t_cosmic_current_snap - t_cosmic_merger)\n",
    "                score += np.exp(-delta_t / tau)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b70b2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merger_score_pre(\n",
    "    target_halo: int,\n",
    "    snapshot: int,\n",
    "    HaloID: np.ndarray,\n",
    "    Snap_coll: np.ndarray,\n",
    "    N_window: int = 100,\n",
    "    tau: float = 5.0,\n",
    ") -> float:\n",
    "    \n",
    "    merger_snaps = Snap_coll[HaloID == target_halo]\n",
    "\n",
    "    score = 0.0\n",
    "    t_cosmic_current_snap = get_snaptime(snapshot)\n",
    "    if len(merger_snaps)>=1:\n",
    "        for merger_snap in merger_snaps:\n",
    "            if merger_snap in range(snapshot - N_window, snapshot+ 1):\n",
    "                t_cosmic_merger = get_snaptime(merger_snap)\n",
    "                delta_t = np.abs(t_cosmic_current_snap - t_cosmic_merger)\n",
    "                score += np.exp(-delta_t / tau)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7963991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_ratio(cat_name, halo_id_snap):\n",
    "    with h5py.File(cat_name, 'r') as f:\n",
    "        SubhaloGrNr_snap = f['Subhalo/SubhaloGrNr'][:]\n",
    "        SubhaloMass_snap = f['Subhalo/SubhaloMass'][:]\n",
    "    \n",
    "    SubhaloMass_in = SubhaloMass_snap[SubhaloGrNr_snap==halo_id_snap]\n",
    "    \n",
    "    if len(SubhaloMass_in) ==1:\n",
    "        mass_ratio = 0\n",
    "    elif len(SubhaloMass_in) ==0:\n",
    "        print(f'check halos {halo_id_snap} at {cat_name}!')\n",
    "        mass_ratio = np.nan\n",
    "    else:\n",
    "        sorted_mass = np.sort(SubhaloMass_in)[::-1]\n",
    "        mass_ratio = sorted_mass[1] / sorted_mass[0]\n",
    "    \n",
    "    return mass_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d911f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fof_id(cat_name, sub_id):\n",
    "    with h5py.File(cat_name, 'r') as f:\n",
    "        SubhaloGrNr_snap = f['Subhalo/SubhaloGrNr'][:]\n",
    "        SubhaloIDs_snap = f['Subhalo/Subhalo_IDs'][:]\n",
    "    \n",
    "    fof_snap = SubhaloGrNr_snap[SubhaloIDs_snap==sub_id]\n",
    "\n",
    "    return int(fof_snap[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e510f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats_labels_dict_tv(fof_halo_id_at99, Center_SubhaloIDs, Target_Halo_IDs, HaloID, Snap_coll, feats_labels_dict, taus, random_state =42):\n",
    "    sub1st_at99 = int(Center_SubhaloIDs[np.array(list(Target_Halo_IDs))==fof_halo_id_at99][0])\n",
    "    feats_labels_dict[fof_halo_id_at99] = {}\n",
    "\n",
    "    with h5py.File(f'/users_path/merger_trace/data/tng_cluster/tng_cluster_mpbs/sublink_mpb_{sub1st_at99}.hdf5', 'r') as mpb_f:\n",
    "        SubfindID = mpb_f['SubfindID'][:]\n",
    "        SnapNum = mpb_f['SnapNum'][:]\n",
    "\n",
    "    for snap in range(72,100):\n",
    "        feats_labels_dict[fof_halo_id_at99][snap] = {}\n",
    "        # get the sorted halo catalog at target snap\n",
    "        cat_name_temp = f'/users_path/merger_trace/data/tng_cluster/tng_cluster_targetcat/targethalo_cat_0{snap}/TargetHalo_MergerCat_0{snap}.hdf5'\n",
    "\n",
    "        # get the progenitor id at target snap\n",
    "        progenitor_id_atmergersnap = SubfindID[SnapNum==snap]\n",
    "\n",
    "        current_redshift =  snaps[snap]['redshift']\n",
    "        \n",
    "        if len(progenitor_id_atmergersnap) >=1:\n",
    "            # get the fof id at target snap\n",
    "            #fof_atmergercat = get(f'https://www.tng-project.org/api/TNG-Cluster/snapshots/{snap}/subhalos/{progenitor_id_atmergersnap[0]}')['related']['parent_halo']\n",
    "            #fofid_atmergercat = int(fof_atmergercat.strip('/').split('/')[-1])\n",
    "            fofid_atmergercat = get_fof_id(cat_name_temp, progenitor_id_atmergersnap)\n",
    "            # print(fof_atmergercat)\n",
    "            \n",
    "            #center_sub_url = get(fof_atmergercat)['child_subhalos']['results'][0]['url']\n",
    "            #second_sub_url = get(fof_atmergercat)['child_subhalos']['results'][1]['url']\n",
    "\n",
    "            #center_mass = get(center_sub_url)['mass']\n",
    "            #second_mass = get(second_sub_url)['mass']\n",
    "            mass_ratio = get_mass_ratio(cat_name_temp, fofid_atmergercat)\n",
    "\n",
    "            features = calculate_all_features_for_snapshot(cat_name_temp, fofid_atmergercat, random_state=random_state)\n",
    "            \n",
    "            label_scores_all = np.zeros(len(taus))\n",
    "            label_scores_pre = np.zeros(len(taus))\n",
    "            for k,tau_temp in enumerate(taus):\n",
    "                label_scores_all[k] = get_merger_score_all(fof_halo_id_at99, snap, HaloID, Snap_coll, N_window=100,tau=tau_temp)\n",
    "                label_scores_pre[k] = get_merger_score_pre(fof_halo_id_at99, snap, HaloID, Snap_coll, N_window=100,tau=tau_temp)\n",
    "\n",
    "            # store results in 3-level nested dict\n",
    "            # every tau has a key\n",
    "            feats_labels_dict[fof_halo_id_at99][snap] = {\n",
    "                'features': features,\n",
    "                'redshift': current_redshift,\n",
    "                'mass_ratio': mass_ratio, #second_mass/center_mass,\n",
    "                **{f'label_score_all_tau{tau_temp:.1f}': score_all for tau_temp, score_all in zip(taus, label_scores_all)},\n",
    "                **{f'label_score_pre_tau{tau_temp:.1f}': score_pre for tau_temp, score_pre in zip(taus, label_scores_pre)}\n",
    "            }\n",
    "\n",
    "            print(f'calculate features and labels for {fof_halo_id_at99} at {snap} where it was {fofid_atmergercat}')\n",
    "    return feats_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/users_path/merger_trace/data/tng_cluster/tng_cluster_catalog/TNG-Cluster_Catalog.hdf5', 'r') as f:\n",
    "    origID_parentsim = f['origID'][:]\n",
    "    haloID__TNGCluster = f['haloID'][:]\n",
    "\n",
    "cat_name ='/users_path/merger_trace/data/tng_cluster/tng_cluster_targetcat/targethalo_cat_099/TargetHalo_MergerCat_099.hdf5'\n",
    "\n",
    "results = Get_HaloIDs(cat_name, SubhaloMassDef='SubhaloMass')\n",
    "\n",
    "(Target_Halo_IDs, Galaxy_nums, Target_Halo_Rs_Crit200, Target_GroupPoses, AvgSFR,\n",
    "Center_SubhaloIDs, Center_SubhaloMasses, Center_SubhaloVelDisp, Center_SubhaloSFR, Center_SubhaloSpin, Center_SubhaloMassRad\n",
    ") = results\n",
    "\n",
    "\n",
    "with h5py.File('/users_path/merger_trace/data/tng_cluster/tng_cluster_cluster_mergers/cluster_mergers.hdf5', 'r') as f:\n",
    "    HaloID = f['HaloID'][:]\n",
    "    Snap_coll = f['Snap_coll'][:]\n",
    "    print(f.keys())\n",
    "\n",
    "\n",
    "HaloID_Mergercat = np.array(list(set(HaloID))).astype(int)\n",
    "\n",
    "\n",
    "Merger_Target_Halo_ID = range(len(haloID__TNGCluster))\n",
    "Merger_Target_Halo_ID = np.array(Merger_Target_Halo_ID)\n",
    "\n",
    "Snap_coll_eachhalo = dict()\n",
    "\n",
    "for i in range(len(Merger_Target_Halo_ID)):\n",
    "    current_halo_id = Merger_Target_Halo_ID[i]\n",
    "    rel_merger_snap = Snap_coll[HaloID==current_halo_id]\n",
    "    halo_id_frommergercat2TNGCluster = haloID__TNGCluster[current_halo_id] \n",
    "    Snap_coll_eachhalo[halo_id_frommergercat2TNGCluster] = rel_merger_snap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da5bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "MergerHaloID_TNGCluster = np.array([haloID__TNGCluster[i] for i in HaloID]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32986b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_labels_dict = {}\n",
    "FOFHaloID = np.array(list(Snap_coll_eachhalo.keys())).astype(int)\n",
    "\n",
    "for i in range(1,len(Snap_coll_eachhalo)):\n",
    "    fofhalo_id = FOFHaloID[i]\n",
    "    # caution!! HaloID is different for TNG-Cluster between merger catalog and fof halo id\n",
    "    feats_labels_dict = get_feats_labels_dict_tv(fofhalo_id, Center_SubhaloIDs, Target_Halo_IDs, MergerHaloID_TNGCluster, Snap_coll, feats_labels_dict, taus=np.linspace(0.1, 4.0, 40), random_state=2025)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Illustris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
