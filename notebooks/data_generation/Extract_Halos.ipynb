{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used for extract particle info of target halos from snapshots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import logging\n",
    "import shutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Physics-related Packages\n",
    "from astropy.cosmology import Planck15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Target_cat_name = \"/users_path/merger_trace/data/tng300/tng300_targetcat/TargetHalo_Cat_092.hdf5\"\n",
    "SnapPath = \"/users_path/merger_trace/data/tng300/tng300_snapshots/\"\n",
    "SavePath = \"/users_path/merger_trace/data/tng300/tng300_halos/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat Null HDF5 Files for Each Target Halo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete files before creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = SavePath\n",
    "\n",
    "if os.path.exists(directory):\n",
    "    # loop over all files/directories in the target directory\n",
    "    for file_or_dir in os.listdir(directory):\n",
    "        file_or_dir_path = os.path.join(directory, file_or_dir)\n",
    "        try:\n",
    "            if os.path.isfile(file_or_dir_path) or os.path.islink(file_or_dir_path):\n",
    "                os.remove(file_or_dir_path)  \n",
    "                print(f\"{file_or_dir_path} has already been deleted\")\n",
    "            elif os.path.isdir(file_or_dir_path):\n",
    "                shutil.rmtree(file_or_dir_path)  \n",
    "                print(f\"{file_or_dir_path} has already been deleted\")\n",
    "        except Exception as e:\n",
    "            print(f\"cannot remove {file_or_dir_path}: {e}\")\n",
    "else:\n",
    "    print(f\"The directory {directory} does not exist\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for Finding Halos with Subhalos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_HalosWithSub(Group_FirstSub):\n",
    "    \"\"\"\n",
    "    return indices of halos in this array\n",
    "    \"\"\"\n",
    "    Indices_HaloWithSUb = np.where( Group_FirstSub != -1)[0]\n",
    "    Indice_CenterHalo = Indices_HaloWithSUb[0]\n",
    "    return Indices_HaloWithSUb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_CenterSubhalo(HaloIDs, SubhaloGrNr, SubhaloIDs):\n",
    "    CenterSubhalo_IDs = np.zeros(HaloIDs.shape)\n",
    "    CenterSubhalo_Indices = np.zeros(HaloIDs.shape)\n",
    "    for i in range(len(HaloIDs)):\n",
    "        HaloID = HaloIDs[i]\n",
    "        CenterSubhalo_Indices[i] = np.array(np.where(SubhaloGrNr == HaloID))[0][0]\n",
    "        CenterSubhalo_IDs[i] = SubhaloIDs[int(CenterSubhalo_Indices[i])]\n",
    "    \n",
    "    return CenterSubhalo_IDs, CenterSubhalo_Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the null files for sky zones\n",
    "# Dict names used here are similar to those in Illustris\n",
    "def create_hdf5_nulldict(Halo_ID):\n",
    "    \"\"\" Given FOF halo IDs, i.e. ID = 0, create a null\n",
    "        hdf5 file to store particles in different FOF halos \"\"\"\n",
    "    file_name = f'{SavePath}snap_halo_{Halo_ID}.h5'\n",
    "    \n",
    "    with h5py.File(file_name, 'w') as hdf:\n",
    "        # create dict for gas particles\n",
    "        PartType0 = hdf.create_group('PartType0')\n",
    "\n",
    "        \"\"\"PartType0.create_dataset(\n",
    "            'CenterOfMass',\n",
    "            shape = (0,3),\n",
    "            maxshape = (None,3),\n",
    "            dtype = 'float64'\n",
    "            )\"\"\" # unit: ckpc/h, Dims: (N,3)\n",
    "        \n",
    "        PartType0.create_dataset(\n",
    "            'Coordinates',\n",
    "            shape = (0,3),\n",
    "            maxshape = (None,3),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: ckpc/h, Dims: (N,3)\n",
    "        \n",
    "        PartType0.create_dataset(\n",
    "            'Masses',\n",
    "            shape = (0,),\n",
    "            maxshape = (None,),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: 10^10 M_{sun}/h, Dims: (N,)\n",
    "\n",
    "        '''\n",
    "        PartType0.create_dataset(\n",
    "            'ParticleIDs',\n",
    "            shape = (0,),\n",
    "            maxshape = (None,),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: None, Dims: (N,)\n",
    "        '''\n",
    "        '''\n",
    "        PartType0.create_dataset(\n",
    "            #Potential is stored in case virial radius is needed\n",
    "            'Potential',\n",
    "            shape = (0,),\n",
    "            maxshape = (None,),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: (km/s)^2/a, Dims: (N,)\n",
    "        '''\n",
    "        '''\n",
    "        PartType0.create_dataset(\n",
    "            'Velocities',\n",
    "            shape = (0,3),\n",
    "            maxshape = (None,3),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: km*square_root(a)/s, Dims: (N,3)\n",
    "        '''\n",
    "        # create dict for DM particles\n",
    "        PartType1 = hdf.create_group('PartType1')\n",
    "        \n",
    "        PartType1.create_dataset(\n",
    "            'Coordinates',\n",
    "            shape = (0,3),\n",
    "            maxshape = (None,3),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: ckpc/h, Dims: (N,3)\n",
    "\n",
    "        '''\n",
    "        PartType1.create_dataset(\n",
    "            'ParticleIDs',\n",
    "            shape = (0,),\n",
    "            maxshape = (None,),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: None, Dims: (N,)\n",
    "        '''\n",
    "        '''\n",
    "        PartType1.create_dataset(\n",
    "            #Potential is stored in case virial radius is needed\n",
    "            'Potential',\n",
    "            shape = (0,),\n",
    "            maxshape = (None,),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: (km/s)^2/a, Dims: (N,)\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        PartType1.create_dataset(\n",
    "            'Velocities',\n",
    "            shape = (0,3),\n",
    "            maxshape = (None,3),\n",
    "            dtype = 'float64'\n",
    "            ) # unit: km*square_root(a)/s, Dims: (N,3)\n",
    "        '''\n",
    "        \"\"\"\n",
    "        we don't have to store DM mass since DM particles\n",
    "        have constant mass\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Halo IDs and halo information\n",
    "with h5py.File(Target_cat_name, 'a') as Target_hdf:\n",
    "    FOF_Halo_IDs =  Target_hdf['Group/FOF_Halo_IDs'][:].astype(int)\n",
    "    GroupFirstSub = Target_hdf['Group/GroupFirstSub'][:]\n",
    "    Group_R_Crit200 = Target_hdf['Group/Group_R_Crit200'][:]\n",
    "    GroupPos = Target_hdf['Group/GroupPos'][:]\n",
    "\n",
    "Target_Halo_IDs = FOF_Halo_IDs[Find_HalosWithSub(GroupFirstSub)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the null files and write logging info\n",
    "\n",
    "log_file = f'{SavePath}create_snap_halo.log'\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_file,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "\n",
    "'''\n",
    "for Halo_ID in Target_Halo_IDs: # we have Halo IDs\n",
    "    try:\n",
    "        create_hdf5_nulldict(int(Halo_ID))\n",
    "        logging.info(f'CREATED {SavePath}snap_halo_{Halo_ID}.h5')\n",
    "    except Exception as e:\n",
    "        logging.error(f'FAILED {SavePath}snap_halo_{Halo_ID}.h5')\n",
    "\n",
    "# Log summary\n",
    "logging.info('HDF5 creation process completed')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Partilces into Each Halo Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Halo IDs and halo information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all the chunks and sort particles\n",
    "# The reason that we have to loop over chunks\n",
    "\"\"\" \n",
    "Note that the truncation of a snapshot in chunks is arbitrary, \n",
    "thus halos may happen to be stored across multiple, subsequent chunks. \n",
    "Similarly, the different particle types of a halo can be stored\n",
    "in different sets of chunks.\n",
    "\"\"\"\n",
    "# sort particles in one chunk\n",
    "def sort_particles(snap_number, chunk_index, Snap_Path, SavePath, \n",
    "                   FOF_Halo_IDs, GroupPos, Group_R_Crit200, GroupFirstSub):\n",
    "    snap_name = f'{Snap_Path}snap_0{snap_number}.{chunk_index}.hdf5'\n",
    "    with h5py.File(snap_name, 'r') as snap_hdf:\n",
    "        # get info for gas particles\n",
    "        # PartType0_CenterOfMass = snap_hdf['PartType0/CenterOfMass'][:]\n",
    "        PartType0_Coordinates = snap_hdf['PartType0/Coordinates'][:]\n",
    "        PartType0_Masses = snap_hdf['PartType0/Masses'][:]\n",
    "        #PartType0_ParticleIDs = snap_hdf['PartType0/ParticleIDs'][:]\n",
    "        # PartType0_Potential = snap_hdf['PartType0/Potential'][:]\n",
    "        #PartType0_Velocities = snap_hdf['PartType0/Velocities'][:]\n",
    "\n",
    "        # get info for DM particles\n",
    "        PartType1_Coordinates = snap_hdf['PartType1/Coordinates'][:]\n",
    "        #PartType1_ParticleIDs = snap_hdf['PartType1/ParticleIDs'][:]\n",
    "        # PartType1_Potential = snap_hdf['PartType1/Potential'][:]\n",
    "        #PartType1_Velocities = snap_hdf['PartType1/Velocities'][:]\n",
    "\n",
    "\n",
    "    # Open each halo files and store corresponding particles in\n",
    "    # find target halo IDs to locate files\n",
    "    Target_Halo_Indices = Find_HalosWithSub(GroupFirstSub)\n",
    "    Target_Halo_IDs = FOF_Halo_IDs[Target_Halo_Indices]\n",
    "    Target_Halo_R = Group_R_Crit200[Target_Halo_Indices]\n",
    "    Target_Halo_Pos = GroupPos[Target_Halo_Indices]\n",
    "\n",
    "    for i in range(Target_Halo_IDs.shape[0]):\n",
    "        Halo_ID = int(Target_Halo_IDs[i])\n",
    "        Halo_R = Target_Halo_R[i]\n",
    "        Halo_x = Target_Halo_Pos[i][0]\n",
    "        Halo_y = Target_Halo_Pos[i][1]\n",
    "        Halo_z = Target_Halo_Pos[i][2]\n",
    "        \n",
    "        Halo_File = f'{SavePath}snap_halo_{Halo_ID}.h5'\n",
    "        with h5py.File(Halo_File, 'a') as halo_hdf:\n",
    "            # The boolean filter to find gas particles in halo\n",
    "            gas_particles_inhalo =  np.array(np.where(\n",
    "                                    (PartType0_Coordinates[:,0] <= Halo_x + Halo_R) &\n",
    "                                    (PartType0_Coordinates[:,0] >= Halo_x - Halo_R) &\n",
    "                                    (PartType0_Coordinates[:,1] <= Halo_y + Halo_R) &\n",
    "                                    (PartType0_Coordinates[:,1] >= Halo_y - Halo_R) &\n",
    "                                    (PartType0_Coordinates[:,2] <= Halo_z + 2*Halo_R) &\n",
    "                                    (PartType0_Coordinates[:,2] >= Halo_z - 2*Halo_R)))\n",
    "                \n",
    "            if gas_particles_inhalo.shape[1]:\n",
    "                print('Halo gas',Halo_ID)\n",
    "                # resize the original datasets to add data conveniently\n",
    "                # get the new size\n",
    "                #print('shape', halo_hdf['PartType0/Masses'].shape)\n",
    "                num_exist_gas = halo_hdf['PartType0/Masses'].shape[0]\n",
    "                num_add_gas = gas_particles_inhalo.shape[1]\n",
    "                new_num_gas = num_exist_gas + num_add_gas\n",
    "                # resize\n",
    "                # halo_hdf['PartType0/CenterOfMass'].resize((new_num_gas,3))\n",
    "                halo_hdf['PartType0/Coordinates'].resize((new_num_gas,3))\n",
    "                halo_hdf['PartType0/Masses'].resize((new_num_gas,))\n",
    "                #halo_hdf['PartType0/ParticleIDs'].resize((new_num_gas,))\n",
    "                # halo_hdf['PartType0/Potential'].resize((new_num_gas,))\n",
    "                #halo_hdf['PartType0/Velocities'].resize((new_num_gas,3))\n",
    "                # use slices command to add data\n",
    "                # halo_hdf['PartType0/CenterOfMass'][num_exist_gas:new_num_gas,:] = PartType0_CenterOfMass[gas_particles_inZone].reshape(-1,3)\n",
    "                halo_hdf['PartType0/Coordinates'][num_exist_gas:new_num_gas, :] = PartType0_Coordinates[gas_particles_inhalo].reshape(-1,3)\n",
    "                halo_hdf['PartType0/Masses'][num_exist_gas:new_num_gas] = PartType0_Masses[gas_particles_inhalo].reshape(-1)\n",
    "                #halo_hdf['PartType0/ParticleIDs'][num_exist_gas:new_num_gas] = PartType0_ParticleIDs[gas_particles_inhalo].reshape(-1)\n",
    "                # halo_hdf['PartType0/Potential'][num_exist_gas:new_num_gas] = PartType0_Potential[gas_particles_inhalo].reshape(-1)\n",
    "                #halo_hdf['PartType0/Velocities'][num_exist_gas:new_num_gas, :] = PartType0_Velocities[gas_particles_inhalo].reshape(-1,3)\n",
    "                del num_exist_gas, num_add_gas, new_num_gas\n",
    "\n",
    "            # The boolean filter to find DM particles in _x_y zone\n",
    "            DM_particles_inhalo =   np.array(np.where(\n",
    "                                (PartType1_Coordinates[:,0] <= Halo_x + Halo_R) &\n",
    "                                (PartType1_Coordinates[:,0] >= Halo_x - Halo_R) &\n",
    "                                (PartType1_Coordinates[:,1] <= Halo_y + Halo_R) &\n",
    "                                (PartType1_Coordinates[:,1] >= Halo_y - Halo_R) &\n",
    "                                (PartType1_Coordinates[:,2] <= Halo_z + 2*Halo_R) &\n",
    "                                (PartType1_Coordinates[:,2] >= Halo_z - 2*Halo_R)))\n",
    "            \n",
    "            if DM_particles_inhalo.shape[1]:\n",
    "                print('Halo_DM',Halo_ID)\n",
    "                # get the new size\n",
    "                #print('shape', halo_hdf['PartType1/Coordinates'].shape[0])\n",
    "                num_exist_DM = halo_hdf['PartType1/Coordinates'].shape[0]\n",
    "                num_add_DM = DM_particles_inhalo.shape[1]\n",
    "                new_num_DM = num_exist_DM + num_add_DM\n",
    "                # resize\n",
    "                halo_hdf['PartType1/Coordinates'].resize((new_num_DM,3))\n",
    "                #halo_hdf['PartType1/ParticleIDs'].resize((new_num_DM,))\n",
    "                # halo_hdf['PartType1/Potential'].resize((new_num_DM,))\n",
    "                #halo_hdf['PartType1/Velocities'].resize((new_num_DM,3))\n",
    "                #use slices command to add data\n",
    "                halo_hdf['PartType1/Coordinates'][num_exist_DM:new_num_DM,:] = PartType1_Coordinates[DM_particles_inhalo].reshape(-1,3)\n",
    "                #halo_hdf['PartType1/ParticleIDs'][num_exist_DM:new_num_DM] = PartType1_ParticleIDs[DM_particles_inhalo].reshape(-1)\n",
    "                # halo_hdf['PartType1/Potential'][num_exist_DM:new_num_DM] = PartType1_Potential[DM_particles_inhalo].reshape(-1)\n",
    "                #halo_hdf['PartType1/Velocities'][num_exist_DM:new_num_DM,:] = PartType1_Velocities[DM_particles_inhalo].reshape(-1,3)\n",
    "                del num_exist_DM, num_add_DM, new_num_DM\n",
    "        \n",
    "    del (# PartType0_CenterOfMass, \n",
    "        PartType0_Coordinates, PartType0_Masses,\n",
    "        #PartType0_ParticleIDs, \n",
    "        # PartType0_Potential, \n",
    "        #PartType0_Velocities, \n",
    "        PartType1_Coordinates,# PartType1_ParticleIDs,\n",
    "        # PartType1_Potential, \n",
    "        #PartType1_Velocities,\n",
    "        gas_particles_inhalo, DM_particles_inhalo)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of chunks. Chunk index starts from 0 \n",
    "# num_chunks = sim['num_files_snapshot']\n",
    "# num_chunks = 3\n",
    "snap_number = 92\n",
    "\n",
    "for chunk_index in range(100, 200): # looping over all the chunks and separate particels into zones\n",
    "    try:\n",
    "        sort_particles(92, chunk_index, SnapPath, SavePath, \n",
    "            FOF_Halo_IDs, GroupPos, Group_R_Crit200, GroupFirstSub)\n",
    "        logging.info(f'sorted {SnapPath}snap_0{snap_number}.{chunk_index}.hdf5')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Failed sorting {SnapPath}snap_0{snap_number}.{chunk_index}.hdf5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
